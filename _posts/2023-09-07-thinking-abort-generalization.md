---
layout: post
title:  "Thinking Abort Generalization"
date:  2023-9-07 
blurb: "关于泛化的一些思考"
---
# 关于泛化的一些思考

## 无监督学习的泛化性能

泛化，一般指的是机器学习的模型在未见过的数据上的性能表现。对于监督学习，我们有PAC学习理论保证我们的泛化性能。但是对于无监督学习——尤其是深度学习领域的无监督学习——我们一直缺乏相关的理论。

对此，ilya提出了一种观点，他认为无监督学习的泛化性能来自于[对柯罗格廖夫压缩器的近似](https://www.youtube.com/watch?v=AKMuA_TVz3A&t=1677s)。

这个结论是16年左右得到的，对于我个人而言这是一个强而有力的结论。但是在我心中，深度学习的泛化性能从何而来依旧是一个未解之谜。

## 对深度学习的观察

对于深度学习，一直以来的存在的一个问题就是：“为什么经过梯度下降训练的神经网络的具有那么强的学习能力？”

对于这个问题，目前仍然没有什么有趣结论。但是对于这个问题有一个很有趣的现象，来自Google的+PAIR组[Do Machine Learning Models Memorize or Generalize?](https://pair.withgoogle.com/explorables/grokking/)。

这篇博客中，+PAIR的成员发现，如果神经网络在**训练集**上的loss为基本为0时，那么采用权重衰减的方式，再持续训练多轮，可以使得神经网络在测试集上的性能有质的飞跃。进过研究发现，神经网络在开始的状态（训练集loss刚刚为0的状态）下过拟合，随后在权重衰减和反向传播的持续作用下，神经网络似乎跳出了局部极小点，找到了更一般的一个泛化规律。这被称为grokking。

对于为什么会有这样的现象，+PAIR给出了一些尝试性质的解释，但是解释的不是很充分。但是这个现象使得我认识到，权重衰减是一个极重要的正则化方法。一般来说，在训练过程中阻碍模型学习的手段，被称为正则化。（L2正则化就是一种常用的正则化方法，当权重衰减中的衰减系数为0.5时，权重衰减和L2正则化是等价的。）

神经网络的泛化性能绝不是单纯的记忆样本就可以得到的，而是正则化和梯度下降在训练过程中，相互拮抗的结果。我们可以说梯度下降是在函数空间中搜索能使得训练集上loss最小的函数。但是目前并不清楚正则化在这里到底起什么作用。

----------------------------
修改于2024-12-30

大约一年后，针对Grokking现象出现了很多有趣的研究，基本上对这种现象为何出现给出了定性的解释。可以说，神经网络中包含了两部分的解，一部分是记忆解，一部分是泛化解，记忆解复杂度高，因而先达到，泛化解复杂度低，在正则化的作用下才能缓慢达到。


Refernce:  
1. [Grokfast](https://arxiv.org/abs/2405.20233) 放大低频率参数的梯度，可以加快grokking，也即加速泛化电路的出现，这是一项被低估的发现。这是一种非常好的正则化。但是这种方法非常接近动量法，不知道实际用起来怎么样。  
2. [GrokComplexity](https://brantondemoss.com/research/grokking/) 柯氏复杂度与grok的关系，这两者存在深层次的因果关系。正则化是非常重要的。  
3. [grokking的机械可解释性分析](https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)  
4. [grokking的详细解释](https://pair.withgoogle.com/explorables/grokking/)  