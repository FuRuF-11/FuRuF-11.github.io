---
layout: post
title:  "Thinking Abort Generalization"
date:  2023-9-07 
blurb: "关于泛化的一些思考"
---
# 关于泛化的一些思考

## 无监督学习的泛化性能

泛化，一般指的是机器学习的模型在未见过的数据上的性能表现。对于监督学习，我们有PAC学习理论保证我们的泛化性能。但是对于无监督学习——尤其是深度学习领域的无监督学习——我们一直缺乏相关的理论。

对此，ilya提出了一种观点，他认为无监督学习的泛化性能来自于[对柯罗格廖夫压缩器的近似](https://www.youtube.com/watch?v=AKMuA_TVz3A&t=1677s)。

这个结论是16年左右得到的，对于我个人而言这是一个强而有力的结论。但是在我心中，深度学习的泛化性能从何而来依旧是一个未解之谜。

## 对深度学习的观察

对于深度学习，一直以来的存在的一个问题就是：“为什么经过梯度下降训练的神经网络的具有那么强的学习能力？”

对于这个问题，目前仍然没有什么有趣结论，但是对于这个问题有一个很有趣的现象，来自Google的+PAIR组[Do Machine Learning Models Memorize or Generalize?](https://pair.withgoogle.com/explorables/grokking/)。

这篇博客中，+PAIR的成员发现，如果神经网络在**训练集**上的loss为基本为0时，那么采用权重衰减的方式，再持续训练多轮，可以使得神经网络在测试集上的性能有质的飞跃。进过研究发现，神经网络在开始的状态（训练集loss刚刚为0的状态）下过拟合，随后在权重衰减和反向传播的持续作用下，神经网络似乎跳出了局部极小点，找到了更一般的一个泛化规律。

对于为什么会有这样的现象，+PAIR给出了一些尝试性质的解释，但是解释的不是很充分。但是这个现象使得我认识到，权重衰减是一个极重要的正则化方法。一般来说，在训练过程中阻碍模型学习的手段，被称为正则化。（L2正则化就是一种常用的正则化方法，当权重衰减中的衰减系数为0.5时，权重衰减和L2正则化是等价的。）

神经网络的泛化性能绝不是单纯的记忆样本就可以得到的，而是正则化和梯度下降在训练过程中，相互拮抗的结果。我们可以说梯度下降是在函数空间中搜索能使得训练集上loss最小的函数。但是目前并不清楚正则化在这里到底起什么作用。