---
layout: post
title:  "thinking-abort-generalization"
date:  2023-9-07 
blurb: "关于泛化的一些思考"
---

泛化，一般指的是机器学习的模型在未见过的数据上的性能表现。对于监督学习，我们有PAC学习理论保证我们的泛化性能。但是对于无监督学习，我们一直缺乏相关的理论。
<br />
<br />
对此，ilya提出了一种观点，他认为无监督学习的泛化性能来自于对柯罗格廖夫压缩器的近似[对泛化的一种观察](https://www.youtube.com/watch?v=AKMuA_TVz3A&t=1677s)。
<br />
<br />
这个结论是16年左右得到的，对于我个人而言这是一个强而有力的结论。但这仍旧不足以解释深度学习泛化性能的由来。
<br />
<br />
对于深度学习，一直以来的存在的一个问题就是：“为什么经过梯度下降训练的神经网络的具有那么强的泛化能力？”
<br />
<br />
对于这个问题，目前仍然没有结论，但是对于这个问题有一个很有趣的现象，来自Google的+PAIR组[Do Machine Learning Models Memorize or Generalize?](https://pair.withgoogle.com/explorables/grokking/)。
<br />
<br />
这篇博客中，+PAIR的成员发现，如果神经网络在训练集上的loss为基本为0时，那么采用权重衰减的方式，再持续训练多轮，可以使得神经网络在测试集上的性能有质的飞跃。进过研究发现，神经网络在开始的状态（训练集loss刚刚为0的状态）下过拟合，随后在权重衰减和反向传播的持续作用下，神经网络似乎跳出了局部极小点，找到了更一般的一个泛化规律。
<br />
<br />
对于为什么会有这样的现象，+PAIR给出了一些尝试性质的解释，但是解释的不是很充分。但是这个现象使得我认识到，权重衰减是一个极重要的正则化方法。一般来说，在训练过程中阻碍模型学习的手段，被称为正则化。L2正则化就是一种常用的正则化方法，另外，当权重衰减中的衰减系数为0.5时，权重衰减和L2正则化是等价的。
<br />
<br />
神经网络的泛化性能绝不是单纯的记忆样本就可以得到的，而是正则化和梯度下降在训练过程中，相互拮抗的结果。我们可以说梯度下降是在函数空间中搜索能使得训练集上loss最小的函数。但是目前并不清楚正则化在这里到底起什么作用，我们并不完全清楚，如果能够探明这点，那将是极好的。
<br />
<br />
总之，神经网络的泛化性能从何而来，仍然有很多需要探究的点。（待续）