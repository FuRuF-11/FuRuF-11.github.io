---
layout: post
title:  "World Model Theory"
date:  2023-11-22
blurb: "世界模型的理论"
---
一直以来，一个问题困扰着我，那就是“Why we need learning”。更具体的问法是，我们可以通过机器学习来创造真正的AGI，实现真正的智能吗？对于这个问题，我受LeCun的世界模型（尽管他觉得我们不需要机器学习）和语言模型的出色表现的启发，得到了一套自己的答案。

先说结论，我认为AGI是可以通过ML来实现的。我的理由来自于对以下两个方面的观察。

首先是LLM的这类自回归语言模型的成功。我们可以看到LLM是具有相当的语言理解能力的，这可以归功于其是在大规模语料数据集上做过预训练。但是让人惊讶的是，他们似乎同样对这个世界有一些了解（GPT4可以理解位置关系、LLM可以使用API作为工具）。当我了解到这一点时，我感到非常的迷惑。训练是在语句上进行的，那么LLM的假设集应当是语义空间（对人类有意义的语句和词汇之间的映射关系的集合）的子集。但是事实并非如此，LLM学习到了部分真实世界的运作规律（同时我认为这才是其泛化能力来源的关键），这远远超出了语义空间的范畴。LLM在语句上做训练，那么理论上它应当只会学习到词语之间的对应关系。但是LLM从这些数据中总结出了一部分世界的运行规律，我认为这正说明了我们的语言是一种对世界的运行规律进行压缩\采样后的总结。更进一步说法是，我们的语言是在我们脑中的世界模型上采样的结果。

例如，“太阳每天在西边落山”这句话暗含了太阳落山的规律。但是这句话并没有涵盖太阳的下落方位、轨迹等所有信息，它不能完全描述太阳落山的运行规律。当我们使用语言描述世界时，我们很难十全十美的描述我们看到的或者想到的一切信息，每一句话的信息熵都是有限的，很难将所有信息压缩进去。但是对于我们自身来说，我们并不是不清楚这些运行规律的信息。因为如果我们想，我们随时可以补充巨量的语句去描述整个太阳下落的过程。

而我认为，这种现象可以用LeCun的世界模型的理论来解释。LeCun的理论是说：我们的脑子里有一个世界模型，对世界上的规律进行了总结，并且可以在这些总结的基础上灵活的做出推理和预测，这才是实现智能的关键。套用到我们的例子中，就是说我们的语言是从我们大脑中的世界模型采样得到的结果，所以每次仅能反应一部分我们脑中的世界模型中的信息（LLM又从我们的语言中学习到了一部分世界模型）。从这个角度来讲，其实我们的意识就是我们的世界模型，我们的意识活动其实就是我们的大脑使用世界模型来进行推理、学习、预测的过程。从这个角度来讲，当我们的模型产生了一个完整的世界模型的时候，就是AGI出现的时刻。

![这是图片](/assets/img/work/post-2023-11-22/1.png "World Moddl")

回到文章最初的问题上，我们何以从上面的这些观察中得到我最初的结论？我认为，既然LLM是从语言上采样并学习即可得到了一部分我们的世界模型，那我们何不直接在真实世界中采样来使得我们的模型获得自己的世界模型？在语料上训练得到的世界模型只能是人类世界模型的子集，要使得模型得到一个和人类同等规模或者超远人类的世界模型，我们必须让我们的模型在整个真实世界中采样。

此时的模型已经不是在单纯在语言上进行训练，而是要在各种模态的信息上进行训练（我认为这似乎是GPT4-V的强大能力的根源）。如果我们能够使得我们的模型足够大且我们对真实世界做了足够多有效的采样（感谢大数定律，可以保证此时采样得到的数据反应了真实的世界的分布），那么从LLM仅在语料上训练就可以得到少部分世界模型这一点来看，只要有合理的训练方法（目前缺少这些方法），那么毫无疑问的，我们就可以使得我们的模型产生世界模型，可以使得AGI真正的出现。