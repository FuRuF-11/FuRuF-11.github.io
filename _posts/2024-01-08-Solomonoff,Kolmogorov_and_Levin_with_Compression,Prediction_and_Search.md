---
layout: post
title:  "Solomonoff,Kolmogorov and Levin with Induction, Compression, and Search"
date:  2024-01-08
blurb: "所罗门诺夫、柯尔莫哥洛夫和莱文与归纳、压缩和搜索"
---
# 所罗门的遗产

将时间调回到1956年的达特茅斯的夏天的末尾，来自于各方的学术巨擘们刚刚结束了为期一个月（会议自8月31日开始）的对于人工智能的讨论，他们当中的大部分都已经离开。

但是，有三个人留了下来（实际上，这三个人一整个夏天都呆在那里进行讨论）。这三个人分别是[马文·明斯基](https://zh.wikipedia.org/wiki/马文·闵斯基)、[约翰·麦卡锡](https://zh.wikipedia.org/wiki/约翰·麦卡锡)还有[雷·所罗门诺夫](https://en.wikipedia.org/wiki/Ray_Solomonoff)。

对于前两者，所有对AI有所学习有所了解的人都不会陌生，但是对于雷·所罗门诺夫（以下简称所罗门），大部分人却闻所未闻。这其实是由于所罗门一生的研究领域，看起来实在是太过抽象、太过偏门且**似乎不像同会议其他人的成果那么有用**。尽管所罗门本人是人类历史上首位使用概率论进行机器学习研究的学者，但是他的主要成就是一门名叫算法概率（Algorithmic probability）——后被发展为算法信息理论（Algorithmic Information Theory）——的古怪学科和关于通用归纳推理的数学理论（General Theory of Inductive Inference）。

但实际上，这位不为人知的雷·所罗门诺夫所提出的理论，正是为如今最伟大的一群AI研究者们（如[Ilya Sutskever](https://en.wikipedia.org/wiki/Ilya_Sutskever)、[Wojciech Zaremba](https://en.wikipedia.org/wiki/Wojciech_Zaremba)、[Marcus Hutter](https://en.wikipedia.org/wiki/Marcus_Hutter)等）指明了研究方向的伟大理论。换句话说，一些世界上最顶尖的头脑真心认为，所罗门的理论是一种**可以被用于实现AGI的理论**。

## AGI的实现理论

我们需要明确的一点是，对于这些人来说，AGI的定义是什么？尽管在学术界并没有确切的、公认的对AGI的定义，但是根据所罗门的理论，我认为可以这样定义AGI：**一种可以根据给定信息对任意给定可计算问题进行高效的求解机器**。

首先让我解释这个定义。对于普罗大众来讲，他们对于AI的期望无外乎一个具有意识的，可以和他们交谈并解答问题，替代他们的日常工作的机器人。我们可以看到GPT就是基本满足了这些需求的一个聊天机器人。人们在使用GPT时往往都是给出问题并希望GPT去回答。这实际上就是利用GPT来自动化的搜索自己提出的问题的解。GPT的回答中的那些客套话和特殊的文法格式，实际上也是在回答OpenAI预设的问题。

即使将现代的AI技术应用在机器人、化学、数学、生物学等各个领域中，我们也实际上是在使用AI在这些领域中去搜索一些特定的问题的解，例如：如何证明拉格朗日定理、蛋白质如何折叠、如何产生新的材料、机器人的位姿动作应该如何调整等等。所以上文中提到的定义实际上是可以涵盖如今我们所有对AGI的期望的。

也许会有人质疑这个定义并没有涉及智能和意识，这似乎与我们期望的真正的人类智能想去甚远。对于这种质疑，我的理论是：人类的智能和意识，是我们的神经系统在亿万年的进化中，就是通过搜索的方式，在求解“活下去”这个问题的过程中，得到的副产物。所以如果我们设置恰当的问题（或者说奖励函数），我们也可以在我提出的这个AGI的定义下得到智能和意识。

那么。为什么说所罗门的理论是AGI的实现理论？实际上，所罗门身前所研究的理论的正式名称是“**Solomonoff`s Induction Inference(SI，所罗门诺夫归纳推理法)**”。这是一种形式化的相当高效的归纳推理理论。从更一般的归纳推理理论的角度来看来看，人脑的思维活动、各种基于统计的传统ML算法、人工神经网络（ANN），本质上都是在给定的数据下进行归纳推理。而我们可以证明，许多的归纳推理方法实际上是SI的一个特例（虽然SI本质上是不可计算的）。并且我们有理由相信，人脑和ANN实际上也属于这些特例。比较SI和人脑，我们可以许多相同之处：
1. 在面对不同的问题时，人脑和SI实际上都对简单的回答更偏好（这被称为奥卡姆剃刀原则）。
2. 人脑在进行推理时和SI一样都会保留对一件事情的多种解释（多重选择原理）。
3. 人脑、ANN、SI实际上都在进行程序搜索。

下面详细描述SI的具体内容和其与AGI理论的关系。

## 柯尔莫哥洛夫复杂度与通用先验

如果要使用归纳推理，那么我们必须使用贝叶斯律。但是根据贝叶斯公式：

$$
P(x|y)=\frac{P(xy)}{P(x)}=\frac{P(y)P(y|x)}{P(x)}
$$

我们必须知道$P(y)$，也即我们的先验概率。一般的，我们需要对这个概率进行假设，然后才能进行推理。

SI将所有归纳推理问题总结为一个字符串外推问题（也即给定一系列符号，预测下一个会是哪一个），并给出了基于奥卡姆剃刀原理的一种先验概率。

$$
P(y)=2^{-K(\mu(y))}
$$

其中，$\mu()$是指一个可以产生y的程序，$K()$指柯尔莫哥洛夫复杂度。

我们可以证明，这个先验概率随着给定的字符串长度n的增加（也即推理前信息的增加），其与真实分布之间的差距，以比$\frac{1}{n}$更快的速度趋近于0。这个先验被称为**通用先验概率**。因此这个先验概率是相当高效率的。但是高效率的代价是，$K()$是不可计算的。

$K()$的名字有很多：柯尔莫哥洛夫复杂度，描述复杂度，柴廷复杂度……。但是最主流的称呼为柯尔莫哥洛夫复杂度（虽然这个概念最早实际上由所罗门诺夫提出，但是柯尔莫哥洛夫使其流行）。通俗的来讲，我们可以这样理解这个概念：我们宇宙中所有的字符串，都是由某些程序产生的，那么这些程序的长度，衡量了这些字符串的复杂度和信息量。

例如，我们都见过芒德布洛特集在复平面上形成的美丽图案。




## 不可计算的归纳推理与莱文通用搜索

采用所罗门诺夫的理论作为一般性假设的最美妙的地方就在于，他解释了神经网络为何会有如此强的泛化性能。因为按照理论，最小化loss的过程，可以看作搜索柯氏复杂度最小程序的过程，而柯氏复杂度最小的程序是唯一的，且一定是泛化性能最好的。

## Compression=Intelligence



## 结语

令人遗憾的是，雷·所罗门诺夫，这位伟大的思想家、AI领域的先驱，已经于2009年永远的离开了我们。但是我可以确定，他深邃而超前的思维将永远伴随着我们，帮助我们捧起AGI的圣杯。R.I.P. 








**Referring:**  
[所罗门诺夫的个人网站](https://raysolomonoff.com/)   
[所罗门诺夫的归纳推理理论](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference)  
[算法概率](https://en.wikipedia.org/wiki/Algorithmic_probability#Overview)  
[柯尔莫哥洛夫复杂度](https://en.wikipedia.org/wiki/Kolmogorov_complexity)  
[译作——老师柯尔莫哥洛夫的生平和工作（9）：1960年代之算法信息论，算法概率论，柯尔莫哥洛夫复杂度（Kolmogorov complexity）](https://zhuanlan.zhihu.com/p/425376986)  
[莱文通用搜索](https://steemit.com/steemstem/@markgritter/leonid-levin-s-universal-algorithm)  
[通用搜索](http://www.scholarpedia.org/article/Universal_search)